{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation on Word2Vec\n",
    "Glenn Abastillas | 24 March, 2020\n",
    "\n",
    "This notebook goes over an example implementation of Word2Vec and some existing packages that perform Word2Vec training.\n",
    "\n",
    "Contents\n",
    "  1.  Preliminary Steps\n",
    "      * [Load Packages](#load_packages)\n",
    "      * [Preprocess Data](#preprocess_data)\n",
    "      * [Quick Background](#quick_background)\n",
    "  2. [Implementation from Scratch](#implementation_from_scratch)\n",
    "      * Word2Vec Flavors: Continuous Back of Words (CBOW) / Skip Grams (SG)\n",
    "      * Training\n",
    "      * Retrieving the trained matrix\n",
    "  3. Using an Existing Package\n",
    "\n",
    "__At each step, we will also cover other packages that can be used to acheive the same thing (e.g., Countvectorizer)__\n",
    "  \n",
    "---\n",
    "\n",
    "### Load Packages <a id=\"load_packages\"></a>\n",
    "First we import packages and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import tqdm\n",
    "from string import punctuation\n",
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from the `gutenberg` corpus and normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "### Preprocess Data <a id=\"preprocess_data\"></a>\n",
    "##### Normalize Vocabulary\n",
    "\n",
    "To improve processing and richness of our lexical items, we normalize our language data. \n",
    "\n",
    "Normalizing data can involve a variety of tasks depending on the final application of our language model. These tasks including making all words the same case, removing punctuation, and removing **<a id=\"stopword\" style=\"text-decoration: none; cursor: help;\" title=\"Words that contribute little semantic information to a text\">stopwords</a>**.\n",
    "\n",
    "For this presentation, we will use **<a id=\"token\" style=\"text-decoration: none; cursor: help;\" title=\"Combinations of characters separated by spaces (e.g., words, numbers)\">tokens</a>** that are not punctuation nor stopwords.\n",
    "\n",
    "Let's quickly define some functions we will use to pare our text data down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "def is_stopword(token):\n",
    "    ''' Check if a specified token is a stopword. \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "            token (str) : token to check\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            (boolean) True if token is a stopword, else False\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "            Input that are not strings will return as False\n",
    "    '''\n",
    "    try:\n",
    "        return token.lower() in stopwords_\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_valid_token(token):\n",
    "    ''' Check if token is valid, i.e., not a stopword or punctuation\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "            token (str) : token to check\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            (boolean) True if token is a stopword, else False\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "            Input that are not strings will return as False\n",
    "    '''\n",
    "    try:\n",
    "        return token.isalnum() & ~is_stopword(token)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Normalization Step\n",
    "Next, we create our `raw_text` data using the functions we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normalized_sents = [[word.lower() for word in sent if is_valid_token(word)] for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our `normalized_text`, we can create a `dict` to convert the strings into numbers for faster processing down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_text = [word for sent in normalized_sents for word in sent]\n",
    "\n",
    "VOCAB, INDEX = np.unique(flattened_text, return_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a `to_index` function to convert strings to integers for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(token, reference=VOCAB):\n",
    "    ''' Convert an input token into an integer according to a specified reference\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            token (str) : token to convert to an integer\n",
    "            reference (list, array) : Reference array with unique vocabulary\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            An integer representing the input token's position in the reference object\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "            Function assumes input token is already in lower case\n",
    "    '''\n",
    "    return np.argwhere(reference == token)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Normalization Step\n",
    "\n",
    "Using the conversion function defined above, we can convert our `normalized_sents` into `data`, which contains only integers that will be used in our Word2Vec example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = ([to_index(word) for word in sent] for sent in normalized_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "### Preprocessing Text\n",
    "\n",
    "Use this reference for later [Dimensions greater than 300 have diminishing returns](https://www.aclweb.org/anthology/D14-1162/)\n",
    "\n",
    "### Quick Background <a id=\"quick_background\"></a>\n",
    "#### Background Into Word2Vec\n",
    "\n",
    "* What is it?\n",
    "  - Quick definition (implementation of theoretical matrix bit)\n",
    "  - What is does to text.\n",
    "  - What the output is.\n",
    "  \n",
    "* Why do we need it?\n",
    "  - Many uses in AI.\n",
    "  - Usage in NLP\n",
    "  - Pros\n",
    "  - Cons\n",
    " \n",
    "* What cool things can it do?\n",
    "  - Condense text into a lightweight matrix\n",
    "  - Provide semantic abilities\n",
    "  - Enable data to have algebraic properties\n",
    " \n",
    "* What are competing models?\n",
    "  - Other models to represent text\n",
    "  - GloVe\n",
    "  - Other vectorization models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "## Implementation from Scratch <a id=\"implementation_from_scratch\"></a>\n",
    "\n",
    "For things example, we will create a Word2Vec language model using the data we preprocessed above. \n",
    "\n",
    "In this section, we will develop a **<a id=\"cbow\" style=\"text-decoration: none; cursor: help;\" title=\"Continuous Bag of Words\">CBOW</a>** flavored Word2Vec model.\n",
    "\n",
    "We will:\n",
    "  * Create CBOW windows\n",
    "  * Create preliminary <a id='one-hot' style='text-decoration: none; cursor: help;' title='A vector that is comprised of zeros and ones indicating absence or presence of a value'>one-hot vectors</a>\n",
    "\n",
    "\n",
    "###### Parameters <a id='parameters'></a>\n",
    "First we define some hyperparameters that we use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'window_size' : 2, 'dimensions' : 100, 'learning_rate' : 0.02, 'epochs' : 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table quickly describes what each parameter does.\n",
    "\n",
    "Parameters | Data Type | Description\n",
    "--- | :-: | :--\n",
    "`window_size` | `int` | The number of target tokens before and after a central token to include\n",
    "`dimensions` | `int` | The number of dimensions in hidden layer. Dimensions greater than 300 have diminishing returns `[cite]`.\n",
    "`learning_rate` | `float` | How quickly our model will correct itself\n",
    "`epochs` | `int` | The number of rounds the model is trained\n",
    "\n",
    "###### Creating the Training Data <a id=\"creating_the_training_data\"></a>\n",
    "\n",
    "**Windowing** : We will generate loose <a id='one-hot' style='text-decoration: none; cursor: help;' title='A vector that is comprised of zeros and ones indicating absence or presence of a value'>one-hot vectors</a> that will serve as input and target data when training our model.\n",
    "\n",
    "First we filter our data to ensure we have sufficient data to window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = [sent for sent in data if len(sent) >= parameters['window_size'] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate our one hot vectors using the `VOCAB` as a model for our vector.\n",
    "\n",
    "Let's define a few functions to help use generate these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(this=1, was=2, that=3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vector = namedtuple('Vector', 'this was that'.split())\n",
    "Vector(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(token, size=VOCAB.size):\n",
    "    ''' Convert an input token into an integer according to a specified reference\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            token (str) : token to convert to a one-hot array\n",
    "            size (int) : length of one-hot array to return\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            (np.array) A one-hot vector\n",
    "    '''\n",
    "    vector = np.zeros(VOCAB.size)\n",
    "    vector[token] = 1\n",
    "    return vector\n",
    "\n",
    "def process_sentence(sent, window_size=parameters['window_size'] + 1):\n",
    "    ''' Return a dictionary with token keys and contexts as values\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            sent (list) : sentence to convert into one-hot vectors \n",
    "                          and group into target and context\n",
    "            window_size (int) : Window size of CBOW\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            (dict) object with tokens as keys and their corresponding\n",
    "                   one-hot vector targets and context in the following format:\n",
    "            \n",
    "            >> { token : [[target, [context-1, context-2, ...]], ...]}\n",
    "    '''\n",
    "    processed_sentence = {}\n",
    "    context = []\n",
    "    \n",
    "    for i, token in enumerate(sent):\n",
    "        before = sent[max(i-window_size, 0):i]\n",
    "        after = sent[i:i+window_size]\n",
    "\n",
    "        for context_token in before + after:\n",
    "            context.append(one_hot_vector(context_token))\n",
    "        \n",
    "        target = one_hot_vector(token)\n",
    "        processed_sentence[token] = [target, context]\n",
    "        context = []\n",
    "    return processed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5183: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 4684: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 10185: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 8669: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 6541: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 11748: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 6790: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 9528: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 4876: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 6286: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 5758: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 8398: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 9831: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]],\n",
       " 16856: [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  [array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "   array([0., 0., 0., ..., 0., 0., 0.])]]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sentence(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.] 9521\n",
      "[0. 0. 0. ... 0. 0. 0.] 4196\n",
      "[0. 0. 0. ... 0. 0. 0.] 7041\n",
      "[0. 0. 0. ... 0. 0. 0.] 9312\n",
      "[0. 0. 0. ... 0. 0. 0.] 82\n"
     ]
    }
   ],
   "source": [
    "vector = np.zeros(VOCAB.size)\n",
    "\n",
    "for datum in data[:1]:\n",
    "    for word in datum:\n",
    "        print(one_hot_vector(word), word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Existing Package\n",
    "\n",
    "There are existing implementations that already exist that allow you to use Word2Vec technology out of the box. In this section, we look at the Python package `gensim` with an API implementing Word2Vec.\n",
    "\n",
    "Contents\n",
    "  * SpaCy\n",
    "  * gensim\n",
    " \n",
    " \n",
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
