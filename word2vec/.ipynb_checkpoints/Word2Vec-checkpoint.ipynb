{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation on Word2Vec\n",
    "\n",
    "--- \n",
    "Glenn Abastillas | 24 March, 2020\n",
    "\n",
    "<link rel=\"stylesheet\" href=\"./css/style.css\" type=\"text/css\" />\n",
    "<img src=\"./images/splash_image.png\" style=\"height: 50%; width: 50%; float: right; margin-left: 2em;\" />\n",
    "\n",
    "This notebook goes over an example implementation of Word2Vec and some existing packages that perform Word2Vec training.\n",
    "\n",
    "Contents\n",
    "  1.  Preliminary Steps\n",
    "      * [Load Packages](#load_packages)\n",
    "      * [Preprocess Data](#preprocess_data)\n",
    "      * [Quick Background](#quick_background)\n",
    "  2. [Implementation from Scratch](#implementation_from_scratch)\n",
    "      * Word2Vec Flavors: Continuous Back of Words (CBOW) / Skip Grams (SG)\n",
    "      * Training\n",
    "      * Retrieving the trained matrix\n",
    "      * Applications\n",
    "  3. Using an Existing Package\n",
    "\n",
    "---\n",
    "\n",
    "### Load Packages <a id=\"load_packages\"></a>\n",
    "First we import packages and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import brown, gutenberg, stopwords\n",
    "from collections import namedtuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML('./style/style.html')\n",
    "alt.renderers.enable('notebook')\n",
    "corpus = brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from the `gutenberg` corpus and normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 131\n",
      "Number of words: 2,332\n"
     ]
    }
   ],
   "source": [
    "title = 'cp01'\n",
    "sents = corpus.sents(title)\n",
    "words = corpus.words(title)\n",
    "\n",
    "print(f'Number of sentences: {len(sents):,}')\n",
    "print(f'Number of words: {len(words):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word encodings and word probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in words]\n",
    "\n",
    "WORDS, COUNTS = np.unique(words, return_counts=True)\n",
    "\n",
    "PROBS = COUNTS**0.75 / (COUNTS**0.75).sum()\n",
    "\n",
    "INDEX = np.arange(WORDS.size)\n",
    "\n",
    "VOCAB = dict(zip(WORDS, INDEX))\n",
    "VOCABR = dict(zip(INDEX, WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Visual Inspection\n",
    "\n",
    "A quick look into our corpus reveals that the most frequent sentences in our corpus are 3, 16, and 10 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(sent) for sent in sents]\n",
    "lengths_data = pd.DataFrame(lengths, columns=['Sentence Length'])\n",
    "lengths_data = lengths_data['Sentence Length'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the counts of sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Counts')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'kind' : 'bar', 'figsize' : (20, 5), 'title' : f'Tally of Sentence Lengths'}\n",
    "ax = lengths_data.plot(**params)\n",
    "ax.set_xlabel('Number of tokens in a sentence')\n",
    "ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### Preprocessing the Text\n",
    "\n",
    "Next we preprocess the text to get rid of overly frequent words (e.g., `the`) and stopwords (e.g., `and`, `but`). \n",
    "\n",
    "\\[INSERT IMAGE HERE OF CROSSED OUT STOPWORDS\\]\n",
    "\n",
    "These functions help us do that filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "def is_stopword(token):\n",
    "    ''' Check if a specified token is a stopword. '''\n",
    "    try:\n",
    "        return token.lower() in stopwords_\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_valid_token(token):\n",
    "    ''' Check if token is valid, i.e., not a stopword or punctuation '''\n",
    "    try:\n",
    "        return token.isalnum() & ~is_stopword(token)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def subsample(vocabulary, probs=PROBS, sample_rate=0.001):\n",
    "    ''' Return the indices of words to be excluded '''\n",
    "    p_keep = (np.sqrt(probs / sample_rate) + 1) * sample_rate / probs\n",
    "    p_keep[p_keep >= 1] = 1\n",
    "    return p_keep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Normalization Step\n",
    "Next, we create our `raw_text` data using the functions we just defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Subsampling\n",
    "\n",
    "Remove overly frequent words that may not contribute much to vector meanings (e.g., the). First, generate a list of frequent words with a low probability of keeping them (e.g., $P_{keep}(w_i) < 0.75$)\n",
    "\n",
    "<p class='attention banner'>cite this paper</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_keep = subsample(WORDS)\n",
    "exclude = [word.lower() for word in WORDS[p_keep < 0.75]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the sentences with the new `exclude` filter.\n",
    "\n",
    "<p class='attention banner'>Examplain usage of timer?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sents = [[word.lower() for word in sent if (word.lower() not in exclude) and word.isalnum()] for sent in sents]\n",
    "print(f'Number of sentences: {len(sents):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Normalization Step\n",
    "\n",
    "Using the conversion function defined above, we can convert our `normalized_sents` into `data`, which contains only integers that will be used in our Word2Vec example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = (np.array([VOCAB[token] for token in sent]) for sent in sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "\n",
    "Use this reference for later [Dimensions greater than 300 have diminishing returns](https://www.aclweb.org/anthology/D14-1162/)\n",
    "\n",
    "## Quick Background <a id=\"quick_background\"></a>\n",
    "\n",
    "<img src=\"./images/splash_image_2.png\" style=\"float: right; height: 40%; width: 40%; margin-left: 2em;\" />\n",
    "\n",
    "<h3>Word2Vec—What is it?</h3>\n",
    "<p class='content'><span class='bold'>Word2Vec</span> is a method of representing words as numerical vectors. This representation allows us to improve model efficiency and endows words with numeric properties that capture semantic relationships in text like the image to the right.</p>\n",
    "<p class='content'>That means that we convert words built from characters to 'words' built from numbers.</p>\n",
    "<p class='example'>$e.g., \"security\" \\Rightarrow [0.1,0.7,0.3,0.9,0.3]$</p>\n",
    "\n",
    "<h3>What are its uses?</h3>\n",
    "<p class='content'>Because of <span class='bold'>Word2Vec</span>'s uncanny ability to capture hidden patterns in large corpora, <span class='bold'>Word2Vec</span> has many uses for information extraction from a large, unknown text. Here's a short list of applications:</p>\n",
    "<p class='content'></p>\n",
    "<p class='content'></p>\n",
    "- Many uses in AI: Semantic analysis, entity discovery, lexical and lexical relationship analysis, and topic modeling.\n",
    "  - Pros: We can do algebra with words and converting strings to numeric vectors unlocks a host of abilities that we can perform on these numeric vectors.\n",
    "  - Cons: We need a large vocabulary and training can be very time and resources heavy.\n",
    " \n",
    " \n",
    "<p class='warning banner'>Add classic image of king - man + woman = queen but as a rebus</p>\n",
    "* What cool things can it do?\n",
    "  - Condense text into a lightweight matrix\n",
    "  - Provide semantic abilities\n",
    "  - Enable data to have algebraic properties\n",
    " \n",
    "* What are competing models?\n",
    "  - Other models to represent text\n",
    "  - GloVe\n",
    "  - Other vectorization models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "## Implementation from Scratch <a id=\"implementation_from_scratch\"></a>\n",
    "\n",
    "For things example, we will create a Word2Vec language model using the data we preprocessed above. \n",
    "\n",
    "In this section, we will develop a **<a id=\"skip-gram\" style=\"text-decoration: none; cursor: help;\" title=\"Using the a token to predict its surroundings\">Skip-gram</a>** flavored Word2Vec model.\n",
    "\n",
    "We will:\n",
    "  * Create Skip-gram windows\n",
    "  * Create preliminary <a id='one-hot' style='text-decoration: none; cursor: help;' title='A vector that is comprised of zeros and ones indicating absence or presence of a value'>one-hot vectors</a>\n",
    "\n",
    "\n",
    "###### Parameters <a id='parameters'></a>\n",
    "First we define some hyperparameters that we use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'window_size' : 3, 'dimensions' : 100, 'learning_rate' : 0.01, 'epochs' : 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='attention banner'>Style this table later and move to before the cell</p>\n",
    "This table quickly describes what each parameter does.\n",
    "\n",
    "Parameters | Data Type | Description\n",
    "--- | :-: | :--\n",
    "`window_size` | `int` | The number of target tokens before and after a central token to include\n",
    "`dimensions` | `int` | The number of dimensions in hidden layer. Dimensions greater than 300 have diminishing returns `[cite]`.\n",
    "`learning_rate` | `float` | How quickly our model will correct itself\n",
    "`epochs` | `int` | The number of rounds the model is trained\n",
    "\n",
    "##### Creating the Training Data <a id=\"creating_the_training_data\"></a>\n",
    "\n",
    "We will generate loose <a id='one-hot' style='text-decoration: none; cursor: help;' title='A vector that is comprised of zeros and ones indicating absence or presence of a value'>one-hot vectors</a> that will serve as input and target data when training our model.\n",
    "\n",
    "First we filter our data to ensure we have sufficient data to window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [sent for sent in data if sent.size >= parameters['window_size'] + 2]\n",
    "\n",
    "print(f'Number of sentences in data: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate our one hot vectors using the `VOCAB` as a model for our vector.\n",
    "\n",
    "Let's define a few functions to help use generate these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datum = namedtuple('Datum', 'target context'.split())\n",
    "\n",
    "def one_hot(token, size=WORDS.size):\n",
    "    ''' Convert an input token into an integer according to a specified reference '''\n",
    "    vector = np.zeros((size, 1))\n",
    "    vector[token] = 1\n",
    "    return vector\n",
    "\n",
    "def generate_examples(data, examples=[], size=parameters['window_size']):\n",
    "    ''' Create a example data with the structure [(context, targets), ...] '''\n",
    "    for i, sentence in tqdm(enumerate(data), desc='Generating examples', total=len(data)):\n",
    "        for j, token in enumerate(sentence):\n",
    "            before = max(j - size, 0)\n",
    "            after = j + size\n",
    "            skip = j + 1\n",
    "\n",
    "            context = one_hot(sentence[j])\n",
    "            targets = np.append(sentence[before:j], sentence[skip:after])\n",
    "            \n",
    "            _targets = []\n",
    "        \n",
    "            for target in targets:\n",
    "                _targets.append(one_hot(target))\n",
    "                \n",
    "            examples.append((context, _targets))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the sentences to generate `target` and `context` data for training.\n",
    "<p class='attention'>Processing time may vary </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = generate_examples(data)\n",
    "print(f'{len(examples)} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "\n",
    "<p class='tip banner'>New Section</p>\n",
    "\n",
    "### Create Layers <a id='create_layers'></a>\n",
    "\n",
    "These matrices will serve as the layers that surround our `word2vec` layer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.random((WORDS.size, parameters['dimensions']))\n",
    "w2 = np.random.random((WORDS.size, parameters['dimensions']))\n",
    "print(f'Dimensions\\nWeights 1 {w1.shape}\\nWeights 2 {w2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "###### Feed Forward Algorithm\n",
    "\n",
    "The first part of a two part algorithm defining a <a id='learning-step' style='text-decoration: none; cursor: help;' title='A phase where training data are learned and errors are adjusted throughout the model'>learning step</a>. This algorithm introduces our randomly initialized model to its first evidence of real data to learn from. It then predicts a surrounding vocabulary item from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(datum, w1=w1, w2=w2):\n",
    "    ''' Return three matrices corresponding to the prediction, hidden layer, and output '''\n",
    "    hidden = w1.T @ datum\n",
    "    output = w2 @ hidden\n",
    "    prediction = softmax(output)\n",
    "    return prediction, hidden, output\n",
    "\n",
    "def softmax(datum):\n",
    "    ''' Return the an array normalized to a probability '''\n",
    "    e = np.exp(datum - datum.max())\n",
    "    e[np.isnan(e)] = 1\n",
    "    return e / e.sum()\n",
    "\n",
    "def calculate_loss(datum, prediction):\n",
    "    ''' Calculate the cross entropy for the output of a forward pass '''\n",
    "    dimensions = prediction.shape[1]\n",
    "    log = np.log(prediction)\n",
    "    log[np.isnan(log) | (log == 0)] = 1\n",
    "    inner_sum = np.sum(datum * log, axis=0, keepdims=1)\n",
    "    outer_sum = np.sum(inner_sum, axis=1)\n",
    "    return - (1 / dimensions) * outer_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='warning banner'>Old functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(prediction, context):\n",
    "    ''' Return a weights with the summed prediction error '''\n",
    "    error = [np.subtract(prediction, token) for token in context]\n",
    "    return np.sum(error, axis=0)\n",
    "\n",
    "def backpropagate(prediction, hidden, target, context, w1=w1, w2=w2):\n",
    "    ''' Update weight matrices according to output from forward() '''\n",
    "    error = calculate_error(prediction, context)\n",
    "    \n",
    "    w2_delta = np.outer(error, hidden) * parameters['learning_rate']\n",
    "\n",
    "    hidden_error = error.T @ w2\n",
    "    w1_delta = np.outer(target, hidden_error) * parameters['learning_rate']\n",
    "    \n",
    "    w1 -= w1_delta\n",
    "    w2 -= w2_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "##### Training Algorithm\n",
    "\n",
    "Having both the feed forward and backpropagation algorithms defined, we can now define a training algorithm to learn all training examples for a single <a id='epoch' style='text-decoration: none; cursor: help;' title='A complete cycle of learning steps through all training data'>epoch</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(examples, w1=w1, w2=w2, parameters=parameters, total=WORDS.size):\n",
    "    ''' Train the Word2Vec model on our training data to generate meaningful word vectors '''\n",
    "    \n",
    "    epochs = trange(parameters['epochs'], desc=\"Training\")\n",
    "    total_loss, loss = [], []\n",
    "    predictions = []\n",
    "\n",
    "    for epoch in epochs:\n",
    "        data = tqdm(examples, desc=f'Epoch {epoch}', leave=False)\n",
    "        for context, targets in data:\n",
    "            prediction, hidden, output = forward(context)\n",
    "            backpropagate(prediction, hidden, context, targets)\n",
    "            loss.append(calculate_loss(context, prediction)[0])\n",
    "        total_loss.append(np.mean(loss))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "###### Test Iteration <a id='test_iteration'></a>\n",
    "\n",
    "This cell loops through all our training data to demonstrate what happens in one training <a id='epoch' style='text-decoration: none; cursor: help;' title='A complete cycle of learning steps through all training data'>epoch</a>.\n",
    "\n",
    "<p class='warning banner'>Processing time may vary</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters['epochs'] = 30\n",
    "loss = train(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the Cross-Entropy Loss of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y = (range(len(loss)), loss)\n",
    "ax = plt.plot(*x_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='tip banner'>Save model with today's date YYYY-MM-DD</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# np.save(f'{datetime.today():%Y%m%d}-model.csv', weights_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking at Some Examples\n",
    "\n",
    "We define some functions to inspect the output of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(token, model=w1):\n",
    "    ''' Return the word vector corresponding to a token '''\n",
    "    return model[token]\n",
    "\n",
    "def similarity(vector1, vector2):\n",
    "    ''' Return the cosine similarity score for two tokens input as vectors '''\n",
    "    if isinstance(vector1, str):\n",
    "        vector1 = VOCAB[vector1]\n",
    "\n",
    "    if isinstance(vector2, str):\n",
    "        vector2 = VOCAB[vector2]\n",
    "    \n",
    "    if isinstance(vector1, int):\n",
    "        vector1 = word_vector(vector1)\n",
    "        \n",
    "    if isinstance(vector2, int):\n",
    "        vector2 = word_vector(vector2)\n",
    "        \n",
    "    a = np.dot(vector1, vector2)\n",
    "    b = np.dot(vector1, vector1)\n",
    "    c = np.dot(vector2, vector2)\n",
    "    return a / (np.sqrt(b) * np.sqrt(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect randomly selected words for their similarity score where $similarity \\in [-1,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.random.randint(0, WORDS.size)\n",
    "v2 = np.random.randint(0, WORDS.size)\n",
    "print(f'{VOCABR[v1]} and {VOCABR[v2]}')\n",
    "similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity(392, 393)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'again'\n",
    "v2 = 'against'\n",
    "\n",
    "vocab1 = v1; vocab2 = v2\n",
    "if isinstance(v1, int):\n",
    "    vocab1 = VOCABR[v1]\n",
    "if isinstance(v2, int):\n",
    "    vocab2 = VOCABR[v2]\n",
    "\n",
    "print(f'{vocab1} and {vocab2}')\n",
    "similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the current model trained only on a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = np.random.randint(0, WORDS.size)\n",
    "a, b = w1[token - 1], w1[token]\n",
    "distance = np.dot(a,b) / (np.sqrt(np.dot(a, a)) * np.sqrt(np.dot(b,b)))\n",
    "\n",
    "print(f'Token number {token} is \"{VOCABR[token]}\" along with \"{VOCABR[token-1]}\" with cos score as {distance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('20200326_blake-poems.csv', weights_1, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "#### Using Our Trained Model\n",
    "\n",
    "We can use our trained model to `list things we can do with our Word2Vec model from earlier`.\n",
    "\n",
    "First, we define functions to help do those things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with algebraic interactions of this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[insert navigation here]`\n",
    "\n",
    "---\n",
    "\n",
    "### Visualize Word2Vec\n",
    "\n",
    "We can visualize our word vectors trained using plotting packages like `matplotlib`, `seaborn`, and others. I make use of the `altair` visualization package.\n",
    "\n",
    "<p class='tip'>Preprocessing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare preprocessing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=2)\n",
    "pca50 = PCA(n_components=min(50, w1.shape[0]))\n",
    "tsne = TSNE(n_components=2, perplexity=100)\n",
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the vector weights so that $\\mu = 0$ and $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scale.fit_transform(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality via PCA transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_2 = pca2.fit_transform(scaled_data)\n",
    "pca_data_50 = pca50.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality via TSNE transform.\n",
    "\n",
    "<p class='warning'> Results may vary from run to run</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne_data = tsne.fit_transform(pca_data_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=tsne_data[:,0], y=tsne_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=pca_data_2[:,0], y=pca_data_2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with Altaïr\n",
    "\n",
    "After transforming the high-dimensional data down to 2 and 3 dimensional datasets, we can better visualize what the vectors we trained look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = tsne_data\n",
    "columns = [f'd{i}' for i in range(current.shape[-1])]\n",
    "index = [f'{VOCABR[i]}' for i in range(current.shape[0])]\n",
    "\n",
    "viz_data = pd.DataFrame(tsne_data, columns=columns, index=index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked = chart.mark_point()\n",
    "marked.encode(x=alt.X('d1:Q', title='Dimension'), y='d0:Q', tooltip='index').interactive(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.sents(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape, matrix_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, h, u = forward(vector, matrix_1, matrix_2)\n",
    "pred.shape, h.shape, u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABR[vector.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABR[pred.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Existing Package\n",
    "\n",
    "There are existing implementations that already exist that allow you to use Word2Vec technology out of the box.\n",
    "\n",
    "Examples of these include:\n",
    "  * SpaCy\n",
    "  * gensim\n",
    "  * ELMo\n",
    "  * fasttext\n",
    " \n",
    " \n",
    "### SpaCy\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "References\n",
    "[1] Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
